{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3ef7f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from helpers import *\n",
    "\n",
    "from implementations import *\n",
    "import zipfile\n",
    "import os\n",
    "import time\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a3c75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = 'dataset_to_release.zip'\n",
    "save_path = 'project1/'\n",
    "file = zipfile.ZipFile(zip_path)\n",
    "file.extractall(save_path)\n",
    "file.close()\n",
    "data_path = 'project1/dataset_to_release'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ea533b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, train_ids, test_ids = load_csv_data(data_path, sub_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcf827e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 321)\n",
      "(109379, 321)\n",
      "(328135,)\n",
      "(328135,)\n",
      "(109379,)\n"
     ]
    }
   ],
   "source": [
    "check_array_shapes(x_train, x_test, y_train, train_ids, test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "550abecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = x_train\n",
    "y = y_train\n",
    "ids = train_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08f5d33",
   "metadata": {},
   "source": [
    "According to the information on website, we delete the first 10 columns contains the information unrelated to the final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75f5b2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tX = tX[:, 10:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae102012",
   "metadata": {},
   "source": [
    "Print the ratio of missing values for each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d518360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyze_missing_values(tX, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a7314",
   "metadata": {},
   "source": [
    "As some data means \"not known\", \"refuse to answer\", we replace those number with NaN. For the data means 0 in value, we replace them with 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3f4bf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "values_to_replace_with_nan = [77, 99, 7, 9, 777, 999, 777777, 999999, 98,7777,9999,89]\n",
    "values_to_replace_with_0_001 = [888,88, 8]\n",
    "\n",
    "replace_values(tX, values_to_replace_with_nan, values_to_replace_with_0_001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d2470e",
   "metadata": {},
   "source": [
    "There is some NaN values in the array, we first delete the feartures. Then we do PCA analysis on the remained features and use k-means cluster to cluster and label each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "466219f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 17)\n"
     ]
    }
   ],
   "source": [
    "tX_NoNaN = remove_columns_with_nan(tX)\n",
    "print(tX_NoNaN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f507c4ae",
   "metadata": {},
   "source": [
    "As the dimension of the tX data is high, we first reduce the dimension and then use K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2958a31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(328135, 15)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_components = 15  # Number of principal components to retain\n",
    "X_reduced_feature = perform_pca(tX_NoNaN, n_components)\n",
    "X_reduced_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4240c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135,)\n"
     ]
    }
   ],
   "source": [
    "n_clusters = 10  # Number of clusters\n",
    "cluster_assignment = k_means_clustering(X_reduced_feature, n_clusters)\n",
    "print(cluster_assignment.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ddc68",
   "metadata": {},
   "source": [
    "Now we already know which cluster each sample belongs to, we delete the featrues that contains too much NaN values. Here we delete the feature clolumns with more than 80% NaN value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53009de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(328135, 129)\n"
     ]
    }
   ],
   "source": [
    "missing_value_threshold = 0.2\n",
    "remained_columns = clean_dataset(tX, missing_value_threshold)\n",
    "cleaned_tX = tX[:, remained_columns]\n",
    "print(cleaned_tX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532e994f",
   "metadata": {},
   "source": [
    "For the remained array, the number of NaN for each feture is not too much. Therefore, we replace the NaN value with corresponding cluster median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf671d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tX_filled = fill_missing_with_median(cleaned_tX, cluster_assignment)\n",
    "train_medians = np.nanmedian(tX_filled, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d70ade6",
   "metadata": {},
   "source": [
    "Normalize X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84e20d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_x = normalize(tX_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccb15013",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_value = 1e-1\n",
    "tx_expanded, initial_w = setup_features_and_weights(normalized_x, w_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d698d",
   "metadata": {},
   "source": [
    "We convert -1 to 0 for y, as we use logistic regression for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2a7f9362",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new = np.where(y == -1, 0, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ef1e74f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 done\n",
      "1 done\n",
      "2 done\n",
      "3 done\n",
      "4 done\n",
      "5 done\n",
      "6 done\n",
      "7 done\n",
      "8 done\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Set the hyperparameters\n",
    "gammas = np.logspace(-0.75, -0.67, 3)\n",
    "lambdas = np.logspace(-1, 0, 3)# Regularization parameter\n",
    "k_fold = 10\n",
    "\n",
    "# Call logistic_regression_gd function for logistic regression\n",
    "gamma, lambda_= best_selection(y_new, tx_expanded, initial_w, k_fold, lambdas, gammas)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4da3620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1778279410038923 0.1\n"
     ]
    }
   ],
   "source": [
    "print(gamma, lambda_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ff5dc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loss: 0.2838681457547004\n"
     ]
    }
   ],
   "source": [
    "final_w, final_loss = reg_logistic_regression(y=y_new, tx=tx_expanded, initial_w=initial_w, gamma=gamma, lambda_=lambda_)\n",
    "print(\"Final Loss:\", final_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad32f73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run time: 101.16616606712341\n"
     ]
    }
   ],
   "source": [
    "print('run time:', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "da5d0a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/don_giraffe/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/nanfunctions.py:1095: RuntimeWarning: All-NaN slice encountered\n",
      "  result = np.apply_along_axis(_nanmedian1d, axis, a, overwrite_input)\n"
     ]
    }
   ],
   "source": [
    "tx_test = preprocess_data(x_test,values_to_replace_with_nan, values_to_replace_with_0_001, n_components, n_clusters, missing_value_threshold, remained_columns, train_medians)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "607d6096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.98700167 -1.25850318 -0.81194227 ... -0.80006917 -0.53972661\n",
      " -0.75015857]\n",
      "0.6363503675066515\n"
     ]
    }
   ],
   "source": [
    "raw_predictions = np.dot(tx_test, final_w)\n",
    "predicted_probabilities = 1 / (1 + np.exp(-raw_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caced340",
   "metadata": {},
   "source": [
    "Make predictions on the test set using the final weight obtained from training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83927723",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = predict(tx_test, final_w)\n",
    "y_pred = np.where(test_predictions == 0, -1, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35d4ec98",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_csv_submission(test_ids, y_pred, name='y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca556e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
